{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks - MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "2. [Introduction](#Introduction)\n",
    "3. [Import the MNIST dataset](#Import-the-MNIST-dataset)\n",
    "    1. [Download the dataset locally](#Download-the-dataset-locally)\n",
    "    2. [Load the MNIST dataset](#Load-the-MNIST-dataset)\n",
    "4. [Define the Neural Network Model](#Define-the-Neural-Network-Model)\n",
    "5. [Train the Neural Network Model](#Train-the-Neural-Network-Model)\n",
    "6. [Conclusion](#Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This notebook examines how the neural network algorithm operates in more detail. The neural network algorithm is applied to the MNIST handwriting dataset with the internal operations defined fully below. The defined algorithm is not the most optimal algorithm in terms of execution time, however, it is very useful in terms of helping to understand the inner workings of the algorithm. \n",
    "\n",
    "This notebook primarily follows the basic neural network code as defined by Micheal Nelson in his online pdf `http://neuralnetworksanddeeplearning.com/`. This book was very useful in helping me to initially understand the internal operations of a neural network model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T15:32:02.846577Z",
     "start_time": "2020-08-22T15:32:02.841524Z"
    }
   },
   "outputs": [],
   "source": [
    "# load the necessary libraries\n",
    "import os\n",
    "import random  \n",
    "import gzip\n",
    "import pickle\n",
    "import urllib.request\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T15:32:03.158283Z",
     "start_time": "2020-08-22T15:32:03.154003Z"
    }
   },
   "outputs": [],
   "source": [
    "# define the absolute root path\n",
    "ROOT_PATH = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the dataset locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T15:32:04.206636Z",
     "start_time": "2020-08-22T15:32:04.188442Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a location to store the dataset\n",
    "try:\n",
    "    os.stat(ROOT_PATH + '/data')\n",
    "except:\n",
    "    os.mkdir(ROOT_PATH + '/data') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T15:32:04.814423Z",
     "start_time": "2020-08-22T15:32:04.811394Z"
    }
   },
   "outputs": [],
   "source": [
    "# define the file location and path to store the MNIST dataset\n",
    "url = 'http://deeplearning.net/data/mnist/mnist.pkl.gz'\n",
    "file_name = ROOT_PATH + '/data/mnist.pkl.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T15:33:03.701320Z",
     "start_time": "2020-08-22T15:32:05.314547Z"
    }
   },
   "outputs": [],
   "source": [
    "# download the file from url and save it locally under file_name:\n",
    "# (this may take a few minutes)\n",
    "with urllib.request.urlopen(url) as response, open(file_name, 'wb') as out_file:\n",
    "    shutil.copyfileobj(response, out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T15:33:03.939342Z",
     "start_time": "2020-08-22T15:33:03.925246Z"
    }
   },
   "outputs": [],
   "source": [
    "# define the library which loads the MNIST image dataset\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    The load_data function returns the MNIST data as a tuple \n",
    "    containing the training, validation and test datasets.\n",
    "\n",
    "    The training_data is returned as a tuple with two entries.\n",
    "    The first entry contains the actual training images. This is \n",
    "    a numpy.ndarray with 50,000 entries. Each entry is, in turn, \n",
    "    a numpy.ndarray with 784 values, representing the 28*28 = 784\n",
    "    pixels in a single MNIST image.\n",
    "\n",
    "    The second entry in the training_data tuple is a numpy.ndarray\n",
    "    containing 50,000 entries. Those entries are just the digit\n",
    "    values (0,...,9) for the corresponding images contained in the \n",
    "    first entry of the tuple.\n",
    "\n",
    "    The validation_data and test_data are similar, except each \n",
    "    contains only 10,000 images, respectively.\n",
    "    \"\"\"\n",
    "    \n",
    "    # open the tar file object\n",
    "    f = gzip.open(ROOT_PATH + '/data/mnist.pkl.gz', 'rb')\n",
    "    \n",
    "    # extract the training, validation and test datasets\n",
    "    training_data, validation_data, test_data = pickle.load(f, encoding='latin1')\n",
    "    \n",
    "    # close the tar file object\n",
    "    f.close()\n",
    "    \n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def load_data_wrapper():\n",
    "    \"\"\"\n",
    "    The load_data_wrapper function returns a tuple containing \n",
    "    (training_data, validation_data, test_data). The function \n",
    "    first loads the training, validation and test datasets \n",
    "    using the load_data function.\n",
    "\n",
    "    The training_data is transformed into a list containing \n",
    "    50,000 2-tuples (x, y). x is a 784-dimensional \n",
    "    numpy.ndarray containing the input image. y is a \n",
    "    10-dimensional numpy.ndarray representing the unit vector \n",
    "    corresponding to the correct digit for x.\n",
    "\n",
    "    The validation_data and test_data datasets are transformed \n",
    "    into lists containing 10,000 2-tuples (x, y). In each case, \n",
    "    x is a 784-dimensional numpy.ndarray containing the input \n",
    "    image, and y is the corresponding classification, i.e., the \n",
    "    digit values (integers) corresponding to x.\n",
    "\n",
    "    Therefore, the formats of the training data and the \n",
    "    validation/test data are different, however these formats \n",
    "    turn out to be the most convenient for use in our neural \n",
    "    network code.\n",
    "    \"\"\"\n",
    "    \n",
    "    # import the training, validation and test datasets\n",
    "    tr_d, va_d, te_d = load_data()\n",
    "    \n",
    "    # reshape the training input images into 784 dimensional numpy arrays\n",
    "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "    \n",
    "    # create a classification vector where each classification value is assigned a 1, other values 0\n",
    "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
    "    \n",
    "    # combine the inputs and classification results into a list of tuples for each pair of x,y values \n",
    "    training_data = list(zip(training_inputs, training_results))\n",
    "    \n",
    "    # reshape the input images into 784 dimensional numpy arrays\n",
    "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
    "    \n",
    "    # combine the input images and classification results into a list of tuples for each pair of x,y values\n",
    "    # this time keep the original classification values\n",
    "    validation_data = list(zip(validation_inputs, va_d[1]))\n",
    "    \n",
    "    # reshape the input images into 784 dimensional numpy arrays\n",
    "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
    "    \n",
    "    # combine the input images and classification results into a list of tuples for each pair of x,y values\n",
    "    # this time keep the original classification values\n",
    "    test_data = list(zip(test_inputs, te_d[1]))\n",
    "    \n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def vectorized_result(j):\n",
    "    \"\"\"\n",
    "    Return a 10-dimensional unit vector with a 1 in the jth\n",
    "    position and zeroes elsewhere.  This is used to convert a digit\n",
    "    (0,...,9) into a corresponding desired output from the neural\n",
    "    network.\n",
    "    \"\"\"\n",
    "    \n",
    "    # create a vector of 0s\n",
    "    e = np.zeros((10, 1))\n",
    "    \n",
    "    # replace the jth value with a 1\n",
    "    e[j] = 1.0\n",
    "    \n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T15:33:04.955914Z",
     "start_time": "2020-08-22T15:33:04.042482Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate the training, validation and test datasets\n",
    "training_data, validation_data, test_data = load_data_wrapper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T15:34:36.071976Z",
     "start_time": "2020-08-22T15:34:36.028546Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the network class which implements the stochastic gradient descent learning algorithm \n",
    "# for a feedforward neural network. Gradients are calculated using backpropagation. \n",
    "\n",
    "class Network(object):\n",
    "    \n",
    "    # initalise the neural network    \n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"\n",
    "        The sizes list contains the number of neurons in the\n",
    "        respective layers of the network. For example, if the list\n",
    "        was [2, 3, 1] then it would be a three-layer network, with the\n",
    "        first layer containing 2 neurons, the second layer 3 neurons,\n",
    "        and the third layer 1 neuron.  The biases and weights for the\n",
    "        network are initialized randomly, using a Gaussian distribution \n",
    "        with mean 0, and variance 1. Note that the first layer is \n",
    "        assumed to be an input layer, and by convention we won't set any \n",
    "        biases for those neurons, since biases are only ever used in \n",
    "        computing the outputs from later layers.\n",
    "        \"\"\"\n",
    "        \n",
    "        # define the number of layers\n",
    "        self.num_layers = len(sizes)\n",
    "        \n",
    "        # define the number of neurons in each layer        \n",
    "        self.sizes = sizes\n",
    "\n",
    "        # randomly choose the initial weights and biases\n",
    "        # define the biases associated to each neuron in all layers excluding the input layer\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        \n",
    "        # define the weights, where the weights connect the neurons from two seperate layers\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        \n",
    "    \n",
    "    # define the feedforward function which takes inputs and multiplies them by the weights adds the bias before \n",
    "    # finally computing the the sigmoid of these values\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"\n",
    "        Return the output of the network if a is input.\n",
    "        \"\"\"\n",
    "        \n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a) + b)\n",
    "        return a\n",
    "    \n",
    "    # define the stochastic gradient descent function\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "        \"\"\"\n",
    "        Train the neural network using mini-batch stochastic\n",
    "        gradient descent. The training_data is a list of tuples\n",
    "        (x, y) representing the training inputs and the desired\n",
    "        outputs. The other non-optional parameters are self \n",
    "        explanatory. If test_data is provided then the network \n",
    "        will be evaluated against the test data after each epoch, \n",
    "        and partial progress printed out. This is useful for \n",
    "        tracking progress, but slows things down substantially.\n",
    "        \"\"\"\n",
    "        \n",
    "        # if given a test dataset calculate the number of observations in the test dataset\n",
    "        if test_data: \n",
    "            n_test = len(test_data)\n",
    "            \n",
    "        # define the number of training observations\n",
    "        n = len(training_data)\n",
    "        \n",
    "        # for each epoch\n",
    "        for j in range(epochs):\n",
    "            \n",
    "            # randomly shuffle the training dataset\n",
    "            random.shuffle(training_data)\n",
    "            \n",
    "            # split the training dataset up into equal mini_batch_size sized mini-batches  \n",
    "            # we have a list of mini-batches of len mini_batch_size each containing mini_batch_size\n",
    "            # tuples of x and y values\n",
    "            mini_batches = [training_data[k:k+mini_batch_size]\n",
    "                            for k in range(0, n, mini_batch_size)]\n",
    "            \n",
    "            # for each mini-batch apply a single step of gradient descent\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            \n",
    "            # evaluate the test data results at the end of each epoch\n",
    "            if test_data:\n",
    "                print(\"Epoch {0}: {1} / {2}\".format(j, self.evaluate(test_data), n_test))\n",
    "            \n",
    "            else:\n",
    "                print(\"Epoch {0} complete\".format(j))\n",
    "    \n",
    "    # update the weights and biases of the network (after single iteration)\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"\n",
    "        Update the network's weights and biases by applying\n",
    "        gradient descent using backpropagation to a single mini-batch.\n",
    "        The mini_batch is a list of tuples (x, y), and eta is the \n",
    "        learning rate.\n",
    "        \"\"\"\n",
    "        \n",
    "        # define empty vectors to store the changes in the weights and biases\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        # for each tuple of x and y in the mini-batch\n",
    "        for x, y in mini_batch:\n",
    "            \n",
    "            # call the backpropagation algorithm and calculate the gradient of the cost function at x (i.e. C_x)\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            \n",
    "            # update the weights and the biases (add the changes across all tuples in the mini-batch)\n",
    "            nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "            \n",
    "        # update the weights of each layer\n",
    "        self.weights = [w - (eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        \n",
    "        # update the biases of each layer\n",
    "        self.biases = [b - (eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"\n",
    "        Return a tuple (nabla_b, nabla_w) representing the\n",
    "        gradient for the cost function C_x. nabla_b and\n",
    "        nabla_w are layer-by-layer lists of numpy arrays, similar\n",
    "        to self.biases and self.weights.\n",
    "        \"\"\"\n",
    "        \n",
    "        # define empty vectors to store the changes in the weights and biases\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        # feedforward\n",
    "        \n",
    "        # define the x the activation input, the 784 dimensional vector that is inputted into the network\n",
    "        activation = x\n",
    "        \n",
    "        # list to store all the activations, layer by layer\n",
    "        # add the input layer to the list\n",
    "        activations = [x] \n",
    "        \n",
    "        # list to store all the activation function input z vectors, layer by layer\n",
    "        zs = []\n",
    "        \n",
    "        # for the biases and weights of each layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            \n",
    "            # calculate the activation input z for each neuron in the layer\n",
    "            z = np.dot(w, activation) + b\n",
    "            \n",
    "            # add the activation input z to the zs list\n",
    "            zs.append(z)\n",
    "            \n",
    "            # tranform the activation inputs for the layer through the sigmoid function\n",
    "            # each neurons z value is transformed individually\n",
    "            activation = sigmoid(z)\n",
    "            \n",
    "            # add the activation output to the activations list\n",
    "            activations.append(activation)\n",
    "        \n",
    "        \n",
    "        # backward pass\n",
    "        \n",
    "        # calculate delta^L = nabla_a_C * sigma_prime(z^L)\n",
    "        # find the output error for the final layer\n",
    "        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
    "        \n",
    "        # remember that the change in the cost relative to the change in the biases is equal to delta\n",
    "        nabla_b[-1] = delta\n",
    "        \n",
    "        # remember that the change in the cost relative to the change in the weights is equal to delta * activations \n",
    "        # of the previous layer\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        \n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book. Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on. It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        \n",
    "        # for the previous layers (the second layer)\n",
    "        for layer in range(2, self.num_layers):\n",
    "            \n",
    "            # take the activation input for the second last layer \n",
    "            z = zs[-layer]\n",
    "            \n",
    "            # pass the activation input through the derivative of the activation function\n",
    "            sp = sigmoid_prime(z)\n",
    "            \n",
    "            # pass the error backwards (from the last layer to the second last layer)\n",
    "            # delta^L = (w^(L+1))^T * delta^(L+1) * sigma_prime(z^L)\n",
    "            delta = np.dot(self.weights[-layer+1].transpose(), delta) * sp\n",
    "            \n",
    "            # again set the change in the cost relative to the biases to be the error delta\n",
    "            nabla_b[-layer] = delta\n",
    "            \n",
    "            # set the change in the cost relative to the weights to be the dot product of the previous error \n",
    "            # and activations from the current layer\n",
    "            nabla_w[-layer] = np.dot(delta, activations[-layer-1].transpose())\n",
    "            \n",
    "        return (nabla_b, nabla_w)\n",
    "    \n",
    "    # evaluate the currrent model on the test dataset\n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"\n",
    "        Return the number of test inputs for which the neural\n",
    "        network outputs the correct result. Note that the neural\n",
    "        network's output is assumed to be the index of whichever\n",
    "        neuron in the final layer has the highest activation.\n",
    "        \"\"\"\n",
    "        \n",
    "        # pass the unseen x values through the feedforward algorithm\n",
    "        # argmax finds the index of the max value of the output of \n",
    "        # the final layer\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        \n",
    "        # return the total number of matchs between y and those predicted by the NN\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "    \n",
    "    # define the derivative of the cost function\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"\n",
    "        Return the vector of partial derivatives \\partial C_x / \\partial a \n",
    "        for the output activations. This is the derivative of the quadratic \n",
    "        cost function.\n",
    "        \"\"\"\n",
    "        return (output_activations-y)\n",
    "\n",
    "# activation function\n",
    "# define the sigmoid activation function\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    \n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "# define the derivative of the sigmoid activation function\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    \n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T15:34:38.821676Z",
     "start_time": "2020-08-22T15:34:38.814627Z"
    }
   },
   "outputs": [],
   "source": [
    "# define a 3 layered network, the initial layer containing 784 (28*28) neurons, the middle hidden layer contains 30 \n",
    "# neurons, while the output layer contains 10 neurons\n",
    "net = Network([784, 30, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T15:37:20.045010Z",
     "start_time": "2020-08-22T15:34:40.113776Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 8271 / 10000\n",
      "Epoch 1: 8360 / 10000\n",
      "Epoch 2: 8449 / 10000\n",
      "Epoch 3: 8499 / 10000\n",
      "Epoch 4: 8493 / 10000\n",
      "Epoch 5: 9343 / 10000\n",
      "Epoch 6: 9379 / 10000\n",
      "Epoch 7: 9426 / 10000\n",
      "Epoch 8: 9408 / 10000\n",
      "Epoch 9: 9441 / 10000\n",
      "Epoch 10: 9442 / 10000\n",
      "Epoch 11: 9476 / 10000\n",
      "Epoch 12: 9476 / 10000\n",
      "Epoch 13: 9471 / 10000\n",
      "Epoch 14: 9489 / 10000\n",
      "Epoch 15: 9521 / 10000\n",
      "Epoch 16: 9498 / 10000\n",
      "Epoch 17: 9504 / 10000\n",
      "Epoch 18: 9509 / 10000\n",
      "Epoch 19: 9521 / 10000\n",
      "Epoch 20: 9516 / 10000\n",
      "Epoch 21: 9512 / 10000\n",
      "Epoch 22: 9451 / 10000\n",
      "Epoch 23: 9498 / 10000\n",
      "Epoch 24: 9475 / 10000\n",
      "Epoch 25: 9508 / 10000\n",
      "Epoch 26: 9488 / 10000\n",
      "Epoch 27: 9481 / 10000\n",
      "Epoch 28: 9492 / 10000\n",
      "Epoch 29: 9511 / 10000\n"
     ]
    }
   ],
   "source": [
    "# train the network\n",
    "# (this might take some time)\n",
    "net.SGD(training_data, 30, 10, 3.0, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main aim of this notebook was to delve deeper into the Neural Network algorithm. TensorFlow and in particular Keras both provide a very straightforward entry point when applying Neural Network models to data. However, oftentimes the user may not actually understand the algorithm itself and how it operates. This is where this notebook is very important as I believe it helps a user fully understand how the theory behind a neural network which in my opinion is very important.\n",
    "\n",
    "Examining the results of the algorithm one can see that after 30 epochs the neural network algorithm obtained an overall test classification accuracy of 95%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
